---
layout: post
title: 隐私保护推理，SIGCOMM ‘25 SCX乱读
date: 2025-09-04
---

今年去参加了 SIGCOMM，见到不少老朋友续了旧，也认识了很多新伙伴。同行里有人社交能力拉满，加了一堆微信。我聊天倒不少，但最后能记下来的东西很有限。也没关系，有缘以后再见，慢慢就成了有交集的熟人。

大会上该有的主流话题都有：智算/通算、测量排障、运维自动化、QoE、卫星通信……但让我觉得特别有意思的是 **隐私保护推理**。  
香港中文大学有一篇一作论文 **SCX: Stateless KV-Cache Encoding for Cloud-Scale Confidential Transformer Serving**，看得我挺感兴趣。另一个相关方向是赫尔辛基大学的海报 **Federated Inference: Towards Collaborative and Privacy-Preserving Inference over Edge Devices**。

![SCX’s stateless inference]({{ site.baseurl }}/assets/images/ChatGPT-whisper.png)
<p align="center"><em>Whispering encrypted prayers into the cloud, receiving divine answers only the faithful can decode.</em></p>


说实话，我对密码学、隐私保护本身是一窍不通。但这个问题很打动我。以前没太在意个人数据，觉得“无所谓”。可自己用 LLM 越多，越意识到数字世界里的“我”早就活在云上，而且可能比现实里的我还完整。自己瞎想做外挂记忆系统，其实就是想把一部分阅读、对话、写作的痕迹放在可控的外部存储里，留给自己用。

而 SCX 走得更远：它想用云端的大模型和算力，但一点都不泄露用户的数据。这个需求对于个人和企业都是真实存在的。但我判断短时间内，这类隐私保护推理可能不会成为主流，因为：

- 不只是 LLM 推理，整个数据 **全生命周期** 都得管住：从生产（传感器、交易记录、病历、物流记录、浏览记录），到存储，再到分析，最后进入训练和推理。现在大多数数据交给了云服务。
- 本地 embedding 计算对算力有要求，embedding 模型本身也不小，本地部署有门槛。
- Key exchange 带来的额外延时，对交互性要求高的应用可能太重，比如搜广推这类 one-shot 推理多、KV cache 复用少的场景，摊不掉 overhead。
- SCX 的方案需要云端和用户端配合，但主动权还是在云。除非算力过剩卖不出去，才可能被迫做差异化。否则云厂商靠什么动力去改？靠条款说“我们不存用户数据”，然后用户要么接受，要么接受。

下面从技术角度，简单记下学习 SCX 的笔记。[SCX原文ACM链接](https://dl.acm.org/doi/10.1145/3718958.3750509)

---

## Take-aways

- **目标**：在不显著增加延迟的前提下，实现隐私保护的 LLM 推理服务。通过“无状态 KV-cache”机制，确保云端无法单独继续生成或还原用户输入。
- **威胁模型**：假设 **半诚实双方**（用户持有数据，云端持有模型），双方遵循协议，但可能想从中多学点信息。
- **开销**：在 LLaMA-7B 上推理延迟 <50ms；性能接近明文推理，远优于全同态/多方计算的百倍开销。

这里“stateless”的定义有点特别：  
> 云端若没有用户密钥，其继续生成下一个 token 的概率 ≤ ε。

---

## 怎么实现的？

### 1. 为什么编码方案能保证原始 QKV 权重仍可使用？

Embedding 在用户侧算好，再 encode 扔给云端。这里不是加密 (encrypt)，而是编码 (encode)：在 embedding 空间里做**置换、冗余、加噪声**。  
这些操作是精心设计过的，确保模型权重仍然适用：

- **Permutation**：注意力 Q、K 点积对置换是等变的。云端在打乱顺序上算，用户解码时再恢复。
- **冗余 token**：云端当真 token 处理，用户解码时丢掉冗余槽位。
- **噪声**：线性层会传播噪声，用户知道噪声种子，可以在解码时抵消。

因此满足 `Decode(f(Encode(x))) = f(x)`，云端直接用原始模型参数，在解码后得到的结果等价于明文推理。

---

### 2. 为什么在 Decoding 阶段，中间层 key 的暴露不会泄露用户信息？

Decode 阶段为了性能，头和尾的 KV cache 用 key 锁住，中间层明文交给云端，这样既能加速，也保证云端单独无法推出后续 token。  
那中间层 KV cache 明文会不会泄露信息呢？

- 中间层表示是经过多层非线性变换的高维隐空间，本身不可逆。  
- Prefill 阶段的置换、冗余和噪声进一步增加了不确定性。  
- 即使云端尝试训练“反演模型”，也只能得到统计上的模糊线索。  
- SCX 用编码和噪声把这种风险控制在可接受范围。

---

### 3. 差分隐私界在这个方案中的作用

Prefill 阶段加拉普拉斯噪声，使得不同输入在云端看来分布接近。差分隐私保证：即使攻击者观察中间状态，区分两个输入的能力提升最多是 ε。

**科普小例子：**  
想象你要统计宿舍平均身高。如果直接报出每个人的身高，隐私就泄露了。差分隐私会给每个人的数字加点随机噪声，导致单个人的数据不可见，但平均值依然差不多对。  
在 SCX 里，噪声让云端看到的 embedding 模糊化，保证它难以确定到底是哪条输入。

换句话说，SCX 提供的是一个**信息泄露的上界**：不是说绝对安全，而是给了数学保证，云端最多能多学到多少信息。

---

## 威胁模型的讨论（Discussion）

### 与现实企业云应用的对比

- **现实情况**：Azure, AWS, OpenAI 企业版依赖合同和合规：不进训练集，日志留 30–90 天做审计。  
- **SCX 模型**：用技术手段“强制”云端看不到输入，适合多租户或担心内部暴露的场景。

### 云厂商能否支持？

技术上能做：在 prefill 和首尾层开放接口，管理用户一次性密钥。  
但权衡性能和复杂度，短期更可能出现在高敏感行业或专有部署。

### 企业实际诉求

更像是一个 **LLM 网关**：日志、审计、敏感信息过滤、合规策略、单点登录。  
可以本地或 VPC 部署，比全加密便宜实用。但 embedding 算力问题仍需解决。

### 数据全生命周期的终极问题

企业数据从生产（仓库、传感器）→ 数据仓库 → 分析可视化 → AI 训练与推理，都需要统一治理与隐私保护。  
仓库端治理成熟，但训练/推理还是短板。不能一头极端安全，另一头裸奔。
