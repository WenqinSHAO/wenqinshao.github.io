---
layout: post
title: LLM外部记忆系统，长上下文之外
date: 2025-08-25
---

> **更长上下文**、**KV cache 扩展**, **context engineering**做算法，做AI Infra的大家都在输出。大家在稀疏注意力（比如 [NSA DeepSeek][0.1]、[Recurrent Memory Transformer][0.2]）和各种“内存池”里挖掘极限，核心矛盾却没变：注意力在**扩展性**与**成本**之间有硬伤，怎么拧都难两全。个人naive的爆论是：上下文再长也是有限的。attention机制这样的东西自己暂时没有条件直接上手，不如来想想外部数据存储如何高效地和LLM的working memory对接。

复杂应用里，Agent 需要的是**长期、可演化、可复用**的记忆，不是一次推理时临时堆上下文。这篇笔记试着**从“外部记忆系统”的角度**把脉络串起来：从早年 NTM/DNC、系统级的 DiskANN，一直到近期的 MemOS、Titans、KBLaM、KG-attention、Zep 等。我想回答两个问题：

* **Q1：为什么 LLM/Agent 需要一个独立于注意力机制的外部记忆系统？**
* **Q2：在系统与算法层面，这样的外部记忆应该长成什么样？**

> 🧠 ：
> 如果你的应用是跨会话、跨任务、跨团队长期运行的 Agent，只靠“更长的上下文”会卡在哪里？你最怕的是**能放**（容量/成本）还是**会放**（结构/管理）？

[0.1]: https://arxiv.org/pdf/2502.11089 "Native Sparse Attention by DeepSeek"
[0.2]: https://arxiv.org/abs/2207.06881 "Recurrent Memory Transformer"

---

## 为什么外部记忆是“必要项”

**注意力和推理是强绑定**：每次生成都要“现取现用”上下文，没法在后台慢慢积累。
**外部记忆不一样**：它可以**脱离推理主循环**，长期做**写入/重组/索引优化**，还能被**多个推理过程或多个 Agent 共享**。

从应用角度看，Agent 需要的不只是单轮对话里的“工作记忆”，还要在更长时间里积累：

* **个性化经历（episodic）**
* **时间序列知识（timeline）**
* **过程性技能（procedural）**

这些长期、结构化的东西：
既不适合硬塞进模型权重，也不可能全丢到即时上下文里。**只有独立可伸缩的外部记忆**，才能扛住**持久存储、层级总结、多视角索引、权限隔离**等需求。

> 结论：外部记忆不是“长上下文的补丁”，而是**分工**。
> **让 LLM 专注即时推理；让外部系统负责长期积累与跨任务复用。**

---

## 外部记忆系统需要哪些特性（避免“过度设计陷阱”）

“**The Bitter Lesson**”给的启示是：少靠人工堆 prompt 和启发式，**拥抱可扩展、可训练**的路线。RAG 的实际落地就暴露了问题：pipeline 复杂、对 prompt 编排/检索调参高度敏感，且容易被无关内容**干扰**（见 [The distracting effect][1.1]、[7 failures of RAG][1.2]）。甚至有研究指出，常见 prompt 技巧（few-shot / CoT / self-refine…）在 RAG 里并不总是稳定优于 baseline（[分析见此][1.3]）。

因此，一个**好的外部记忆系统**应该：

1. **可训练、少 prompt**：尽量端到端或模块化训练，少靠复杂的 prompt stacks/heuristics。
2. **系统可扩展、支持共享**：读/写分离，可服务多 LLM/Agent，避免“每个场景重搭一套”。
3. **原生的结构化管理**：层级总结、版本控制、多索引（episodic/timeline/procedural…）应是**系统能力**，不是 prompt 规则。
4. **动态更新与自我优化**：自动识别重要信息、重构记忆结构，少靠手动“留/删”策略。
5. **可解释与可审计**：比“黑盒 prompt”更可追踪的访问路径与更新逻辑。

> 🧩 检查点：
> 你的记忆系统里，有哪些能力现在是靠“人肉+prompt”勉强顶着的？哪些能下沉为**系统能力或可训练模块**？

[1.1]: https://arxiv.org/abs/2505.06914?utm_source=chatgpt.com "The Distracting Effect: Understanding Irrelevant Passages in RAG"
[1.2]: https://arxiv.org/abs/2401.05856?utm_source=chatgpt.com "Seven Failure Points When Engineering a Retrieval Augmented Generation System"
[1.3]: https://arxiv.org/html/2411.19463?utm_source=chatgpt.com "Towards Understanding Retrieval Accuracy and Prompt Quality in RAG Systems"

---

## 回看老工作：它们其实早就给出了“范式提示”

在谈新之前，值得再看三块基石：**NTM、DNC、DiskANN**。当年没直接催生大规模应用，但今天看**启发很直接**。

### 1) [NTM][2.1] / [DNC][2.2]：把“存储”做成可训练的独立模块

* **NTM（2014）**：在网络外引入**可微、可寻址**的外部存储矩阵；通过读写头+控制器交互。
* **DNC（2016）**：更灵活的寻址/内存分配。
* 真正贡献：树立了\*\*“存储与推理解耦”**的范式。**长期知识**不必全靠参数权重；模型可以**学会使用一个独立记忆\*\*。
* 对今天 LLM-Agent 意味着：记忆**可以独立扩展、独立优化**，不被主模型规模绑死。

[2.1]: https://arxiv.org/abs/1410.5401 "NTM"
[2.2]: https://deepmind.google/discover/blog/differentiable-neural-computers/ "DNC"

### 2) [DiskANN][2.3]：从 ANN 到“记忆基础设施”的系统思维

* 展示了如何用**层次化索引 + 硬盘存储**做低延迟的大规模检索。
* 启示：记忆**不必全在内存**；可以“**热内存 + 冷存储**”分层。
* 放到 LLM 里：KV cache 语义、外部知识库、长期记忆如何分层？如何用**异步 IO/批检索**把延迟压下去？DiskANN 的工程心法仍有价值。

[2.3]: https://www.microsoft.com/en-us/research/project/project-akupara-approximate-nearest-neighbor-search-for-large-scale-semantic-search/ "DiskANN"

### 3) 从 toy task 到 Agent 长期记忆：问题其实早被点名

过去大家吐槽 NTM/DNC 只会“复制/排序”的小任务。但它们帮我们**抽象出了关键问题**：

* **写什么、忘什么由谁决定？**
* **寻址/访问如何高效？**
* **如何把计算（controller）与存储（memory）解耦，并提供可训练接口？**

> 小结：
> **NTM/DNC** → 可微外部记忆的**算法范式**；
> **DiskANN** → 可扩展的**系统范式**。
> 未来可能是两条线**汇合**：既要**可训练的记忆控制器**，又要**工程上可扩展的存储系统**。

---

## 跟进近期进展：MemOS / Titans / KBLaM / KG-attention / Zep

> 下面这几项更贴近“把外部记忆带进真实 LLM 场景”的落地探索。

### 1) [MemOS][3.1]：把记忆当“系统服务”（像 OS）

* 思路：记忆从 LLM 本体里剥离，做成**统一 API**，支持**并发读写、隔离、多租户**。
* 优点：系统层复用能力强。
* 短板：写入**策略**还是偏 heuristics（何时写、写多宽），距离**可训练、可自适应**的控制器还有路。

[3.1]: https://arxiv.org/abs/2505.22101 "MemOS"

### 2) [Titans][3.2]：脑启发的多层记忆，**test-time** 也能学

* 设计：短期/长期/持久三层结构 + “**惊讶驱动**”选择机制，推理时动态决定**保留什么**。
* 亮点：更接近人类记忆；**推理时**就能更新，突破“参数冻结”。
* 难点：“惊讶”指标偏 heuristics，如何端到端训出**泛化的控制器**还不容易。

[3.2]: https://arxiv.org/abs/2501.00663 "Titans"

### 3) [KBLaM][3.3]：把知识图谱三元组**token 化**注入注意力

* 近似“无提示的 RAG”：**线性扩展、可动态更新、可解释**。
* 局限：更适合**结构化 KB**；对非结构化/模糊记忆（对话史、episodic）不太够。

[3.3]: https://arxiv.org/abs/2410.10450 "KBLaM"

### 4) [KG-attention][3.4]：不改 LLM 架构，轻量接入 KG

* 通过**双通道注意力**（输入→KG、KG→输入）把外部知识拉进来。
* 优点：**兼容性好**、实时更新。
* 本质：还是“retrieval + attention”，对**长期管理/结构演化**触及不多。

[3.4]: https://arxiv.org/abs/2507.08704 "KG-Attention"

### 5) [Zep][3.5]：面向 Agent 的**时间感知**图结构记忆

* 特点：构建**双时间轴**（Timeline T 与 T’），以 **episode / entity / community** 分层组织长期对话与业务知识。
* 有趣点：**edge invalidation** 让过时事实失效（类似版本控制）。
* 代价：系统复杂，**prompt 编排依赖重**；距离“可训练、自动化控制器”还有距离。

[3.5]: https://arxiv.org/abs/2501.13956 "Zep"

| 工作               | 核心思想                   | 优势                        | 局限                     | 长期潜力 |
| ---------------- | ---------------------- | ------------------------- | ---------------------- | ---- |
| **MemOS**        | 记忆即 OS service，支持并发与隔离 | 系统层能力强、可复用                | 写入策略偏 heuristics、训练性不足 | 高    |
| **Titans**       | 脑启发多层记忆，test-time 动态学习 | 贴近日常记忆、可动态更新              | 惊讶机制难端到端训练             | 高    |
| **KBLaM**        | 三元组→知识 token 注入注意力     | 线性扩展、可更新、可解释              | 只适合结构化 KB              | 中    |
| **KG-attention** | 双通道注意力接入 KG            | 简单、兼容、实时                  | 依赖检索、缺少长期管理            | 中低   |
| **Zep**          | 双时间轴图谱 + 分层记忆          | 版本控制、episodic/semantic 结合 | 系统复杂、prompt 依赖大        | 中高   |

> 小结：
> 系统线（MemOS、Zep）侧重**复用/结构/隔离**；
> 模型线（Titans、KBLaM、KG-attention）侧重**怎么把外部信息进主循环**。
> 我更看好 **Titans（可训练的 test-time 更新）** + **MemOS（系统级共享/隔离）** 的合流。
> Zep 很有野心，但也最容易**过度设计**。

---

## 走向“可训练、模块化”的外部记忆：一个架构设想

> 开一下脑洞：**可训练的 Memory Controller + 分层存储**。它不是最终答案，但至少给出一条可实现的路径。

### 1) **Trainable Memory Controller**

* 职责：**写入、重组、压缩、索引、多级摘要**；动态更新、淘汰与优先级策略。
* 目标：与 LLM 的 **KV cache/注意力接口**无缝衔接，尽量**少 prompt、少规则**。
* 对比当下：许多 RAG 系统靠“新对话就写”“固定窗口检索”这类手工规则，**自适应性弱**。
* 方向：引入**可训练**控制器，根据不确定性、任务目标或“惊讶”信号决定**写/忘**。

  * **Titans** 给出“惊讶驱动更新”的雏形（但仍带 heuristics）。
  * **Ret-LLM** 提出可扩展、可更新、可解释的读写记忆单元，以**三元组**存储知识，不依赖 prompt 组合（[4.1]）。
  * **MLP Memory** 把**预训练 MLP 外部记忆**做成可微组件，替代传统检索器，并与 LLM decoder 解耦（[4.2]）。
* 共同点：把**外部记忆纳入可训练闭环**，让读写策略与主模型**联合优化**——从 heuristics 走向**学习到的机制**。

[4.1]: https://arxiv.org/html/2305.14322v2 "Ret-LLM"
[4.2]: https://arxiv.org/abs/2508.01832 "MPL Memory"

> 🧠 ：
> 你现在系统里“写/忘”的触发信号是什么？能不能用**不确定性/损失上升/惊讶度**之类的信号替换固定阈值？

### 2) **存储结构：热-温-冷 的分层**

* **热（hot）**：超快访问层（KV cache、实时交互）。
* **温（warm）**：结构化/索引化记忆（向量库、知识图谱）。
* **冷（cold）**：长期语义日志、历史会话等持久层。
* 难点：**跨层迁移** + **一致性** + **延迟**。

  * **DiskANN** 证明大规模也能靠**分层索引**打平延迟。
  * **MemOS** 从“记忆即服务”推并发与隔离。
* 现实还要同时支撑**结构化**（DB/KG）与**非结构化**（对话史/episodic）两类存储，并提供**统一接口**。

  * **KBLaM**：把结构化知识**直接 token 化**注入；
  * **Zep**：更像**非结构化对话史的治理**（时间感知、版本控制）。
* 关键挑战：在**同一框架**下融合两类存储，让模型既能做**逻辑推理**，又能保留**个性化 episodic**。

### 3) **接口与多租户**

* 未来外部记忆很可能是**共享基础设施**：多个 LLM/Agent 用**统一 API** 访问，像文件系统/数据库。
* 需要：**标准化接口、权限隔离、版本管理、并发控制**。
* **MemOS** 提供了一个雏形（把记忆抽象为“资源”并调度），但尚缺跨项目/跨平台标准，导致很多实现停留在 **ad-hoc**，不利于复用与规模化。

> ✅ 实践清单（给架构/落地做参考）
>
> * 少堆 prompt：能训就训，能模块化就模块化。
> * 读写分离 + 多租户：一套记忆服务、服务多模型/多 Agent。
> * 分层存储：把**热/温/冷**的边界画清；指标要能驱动**跨层迁移**。
> * 可解释与审计：任何一次“写/忘/改索引”为何发生，要能追溯。
> * 渐进替换 heuristics：先在边缘场景上用可学习信号替换人工规则，再逐步推广。

---

## last catch

**真正的难题**不在“能不能把知识接进来”，而在：**如何在保持可训练性的同时，做到系统可扩展、可复用、可演化**。这决定我们会不会掉进“功能很多但越来越难维护”的黑洞。

我目前的押注方向：

* **Titans** 代表的“**test-time 可训练更新**”；
* **MemOS** 代表的“**系统级共享与隔离**”。
  二者合流，可能就是下一代外部记忆系统的轮廓。
  